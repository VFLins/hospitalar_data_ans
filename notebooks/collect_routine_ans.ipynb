{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5ea841a-a163-4369-b46a-7748e0aba90a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Coletando dados de saúde suplementar da ANS\n",
    "Esta rotina de coleta foi feita para ser compatível com ambientes de computação distribuída, para chegar neste objetivo, será usado:\n",
    "\n",
    "- `PySpark` para rotina de coleta e armazenamento de dados\n",
    "- Oracle Cloud Infrastructure (OCI), modalidade gratuita para Data Warehousing\n",
    "\n",
    "Inicialmente, este projeto foi pensado para executar nas instâncias de computação distribuída gratuitas do **Databicks**, mas isto acabou sendo descartado por causa do método de autenticação escolhido para acessar o banco de dados. Para manter o maior nível de segurança, esta rotina será executada localmente.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9649c1e-2a1b-4e7e-8ed1-6e2c0a40a0e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Dependências\n",
    "\n",
    "Vamos instalar um [pacote desenvolvido por mim](https://pypi.org/project/ftp-download/) para superar o desafio de realizar múltiplos downloads de arquivos em servidores, e na sequência vamos instalar as demais dependências, incluindo o PySpark, que será usado aqui para transportar os dados para o banco de dados remoto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8bd1653-02b8-4e11-bf41-8f4bfc73a265",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%python` not found (But cell magic `%%python` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "%python -m pip install --upgrade -q pip\n",
    "%pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a9effb2-5ca7-4618-a9b0-7af075a70cea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import ftp_download as ftpd\n",
    "from os import path, listdir, makedirs\n",
    "import logging\n",
    "from ftplib import FTP\n",
    "from getpass import getpass\n",
    "from shutil import unpack_archive\n",
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "# sys.path.append('/dbfs/FileStore/tables/collectutils')\n",
    "# import collectutils\n",
    "\n",
    "s = SparkSession.builder.appName(\"dbInteract\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6afada5-d8c2-4378-9f6a-452a89b23fa5",
     "showTitle": true,
     "title": "vflins@outlook.com"
    }
   },
   "outputs": [],
   "source": [
    "TNS_STR = \"(description= (retry_count=20)(retry_delay=3)(address=(protocol=tcps)(port=1522)(host=adb.sa-saopaulo-1.oraclecloud.com))(connect_data=(service_name=g8c67f84bc4a850_demodb_high.adb.oraclecloud.com))(security=(ssl_server_dn_match=yes)))\"\n",
    "\n",
    "DRIVER = \"oracle.jdbc.driver.OracleDriver\"\n",
    "\n",
    "USR = \"ADMIN\"\n",
    "# PWD = getpass(\"Insira a senha de acesso do Administrador do banco de dados: \")\n",
    "# URL = f\"jdbc:oracle:thin:{USR}/{PWD}//adb.sa-saopaulo-1.oraclecloud.com:1522/g8c67f84bc4a850_demodb_high.adb.oraclecloud.com\"\n",
    "URL = f\"jdbc:oracle:thin:@{TNS_STR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebeec307-88d0-4d1f-8a58-adc6cc9855d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Preparando a rotina de coleta\n",
    "\n",
    "Nosso objetivo é conectar à um servidor FTP e baixar uma quantidade considerável de arquivos, o design da rotina abaixo leva em consideração **nossas limitações, que são**:\n",
    "\n",
    "1. Os arquivos são `.csv` compactados dentro de arquivos `.zip`;\n",
    "2. Existem dois tipos de tabelas: \n",
    "  a. *Detalhada*, e\n",
    "  b. *Consolidada*, estas devem compor tabelas diferentes do nosso banco de dados;\n",
    "3. Todos estes arquivos estão separados em pastas, uma para cada estado\n",
    "\n",
    "A **estratégia adotada** consiste em:\n",
    "\n",
    "1. Fazer múltiplos downloads concorrentes usando computação assíncrona;\n",
    "2. Descompactar e organizar os arquivos localmente\n",
    "3. Usar o pyspark para ler e guardar os dados no banco de dados;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac09effa-27d0-46fe-80bb-bdf769d6915b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FTP_SERVER = \"ftp.dadosabertos.ans.gov.br\"\n",
    "ROOT_FOLDER_SRC = \"/FTP/PDA/TISS/HOSPITALAR/2019/\"\n",
    "\n",
    "ftp = FTP(FTP_SERVER)\n",
    "ftp.login()\n",
    "remote_paths = ftp.nlst(ROOT_FOLDER_SRC)\n",
    "\n",
    "print(remote_paths[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b782fc3b-b332-410d-adc0-c8b8d508ea0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Já sabemos que os arquivos no servidor possuem nomes regulares, eles possuem sufixo \"DET\" nos arquivos com a tabela detalhada, e \"CONS\" nas tabelas consolidadas. Sabemos também que todos os arquivos estão compactados com extensão `.zip`.\n",
    "\n",
    "Para superar estes dois desafios, vamos criar uma função que identifica o tipo de tabela pelo nome do arquivo e extrai os conteúdos em um diretório em comum, de modo que todas as tabelas consolidadas estejam armazenadas em uma pasta, e todas as tabelas detalhadas estejam armazenadas em uma outra pasta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebaa1e88-e81f-480b-af92-46a98acab855",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_and_organize(search_dir: str, find_patterns=[\"CONS\", \"DET\"]):\n",
    "    def filter_by_pattern(pattern, elements):\n",
    "        matches = re.compile(pattern, re.IGNORECASE)\n",
    "        return list(filter(matches.search, elements))\n",
    "\n",
    "    filepaths = [\n",
    "        f for f in listdir(search_dir) \n",
    "        if path.isfile(path.join(search_dir, f))\n",
    "    ]\n",
    "    listings = {i:filter_by_pattern(i, filepaths) for i in find_patterns}\n",
    "\n",
    "    for pattern in find_patterns:\n",
    "        destination = path.join(search_dir, pattern)\n",
    "\n",
    "        if not path.exists(destination):\n",
    "            makedirs(destination)\n",
    "\n",
    "        for f in listings[pattern]:\n",
    "            origin = path.join(search_dir, f)\n",
    "            unpack_archive(origin, destination, \"zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da72d120-8cf7-452e-bc92-be96905b07b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A próxima etapa vai envolver o uso de um pacote de minha autoria `ftp_download`, para saber mais sobre o projeto, visite [https://pypi.org/project/ftp-download/](https://pypi.org/project/ftp-download/). A rotina na célula abaixo segue três etapas:\n",
    "\n",
    "1. **Download** dos arquivos para o ambiente local com `ftp_download`\n",
    "2. **Descompactação e organização** dos arquivos baixados em pastas usando a rotina desenvolvida na célula anterior\n",
    "3. **Movimentação dos arquivos** da unidade local para a base de dados NoSQL do databricks `dbfs`, onde o pyspark tem acesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bbd8612-52ec-44ec-87b6-9c8f95689a00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "ftpd.Conf.verbose = False\n",
    "# exibir log apenas com avisos e erros\n",
    "ftpd.timings.log.handler.setLevel(logging.WARNING)\n",
    "ftpd.timings.log.logger.setLevel(logging.WARNING)\n",
    "\n",
    "for i, rp in enumerate(remote_paths):\n",
    "    print(f\"Progresso: {i/len(remote_paths)*100:.2f}%\", end=\"\\r\")\n",
    "    ftpd.from_folder(ftp, remote_path=rp)\n",
    "print(\"=== Concluído! ===\")\n",
    "\n",
    "with open(ftpd.prefs.LOG_FILE) as logfile:\n",
    "    logfile_contents = logfile.read()\n",
    "    print(logfile_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fedb8e1-0999-4241-a24b-3a73bfbcc2ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2\n",
    "download_place = ftpd.Conf.download_folder\n",
    "extract_and_organize(search_dir=download_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3612ff05-f59b-4084-a246-eb03156f174c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3\n",
    "for subfolder in [\"CONS\", \"DET\"]:\n",
    "    dbutils.fs.mv(\n",
    "        f\"file:{path.join(download_place, subfolder)}\", \n",
    "        f\"dbfs:{path.join('/ans/hosp/2019/', subfolder)}\",\n",
    "        recurse=True)\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/ans/hosp/2019/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c4b1f66-0414-4479-9890-a252ada0436e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Preparando a rotina de armazenamento\n",
    "\n",
    "Agora que já temos os dados prontos para manipulação, podemos usar o `pyspark` para inserir os dados em nosso banco de dados relacional da Oracle. Para isto, vamos usar usar o [driver JDBC da Oracle](https://www.oracle.com/br/database/technologies/appdev/jdbc-downloads.html). *Esta etapa vai falhar se o driver __não__ estiver instalado no cluster atual*, para efetuar a instalação, seguimos os passos indicados [neste vídeo](https://youtu.be/3tAVXfIBqA8?si=jNeO459775ag9x44&t=261).\n",
    "\n",
    "Vamos começar definindo um `schema` para todos os nomes de colunas possíveis. Para decidir qual o *data type* ideal para cada coluna, usamos o [dicionário de dados](https://dadosabertos.ans.gov.br/FTP/PDA/TISS/DICIONARIO/Dicionario_de_variaveis.ods) fornecido pela ANS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c125cb16-3aaf-459e-b314-09ff7add7cb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VAR_TYPES = StructType()\\\n",
    "    .add(\"TEMPO_DE_PERMANENCIA\", IntegerType(), True)\\\n",
    "    .add(\"ID_EVENTO_ATENCAO_SAUDE\", IntegerType(), True)\\\n",
    "    .add(\"CD_TABELA_REFERENCIA\", StringType(), True)\\\n",
    "    .add(\"CD_MUNICIPIO_BENEFICIARIO\", StringType(), True)\\\n",
    "    .add(\"CD_MUNICIPIO_PRESTADOR\", StringType(), True)\\\n",
    "    .add(\"UF_PRESTADOR\", StringType(), True)\\\n",
    "    .add(\"CD_CARATER_ATENDIMENTO\", StringType(), True)\\\n",
    "    .add(\"CD_TIPO_INTERNACAO\", StringType(), True)\\\n",
    "    .add(\"CD_REGIME_INTERNACAO\", StringType(), True)\\\n",
    "    .add(\"CD_MOTIVO_SAIDA\", StringType(), True)\\\n",
    "    .add(\"IND_ACIDENTE_DOENCA\", StringType(), True)\\\n",
    "    .add(\"ANO_MES_EVENTO\", StringType(), True)\\\n",
    "    .add(\"CD_PROCEDIMENTO\", StringType(), True)\\\n",
    "    .add(\"CID_1\", StringType(), True)\\\n",
    "    .add(\"CID_2\", StringType(), True)\\\n",
    "    .add(\"CID_3\", StringType(), True)\\\n",
    "    .add(\"CID_4\", StringType(), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee1754d-e06b-43b7-832f-4dea00a24f15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Variáveis de conexão com o banco de dados\n",
    "Agora vamos obter as variáveis que vão nos ajudar a conectar à *\"autonomous database\"* que já temos alocada na Oracle.\n",
    "\n",
    "- A `TNS_STR` é providenciada pela OCI como uma opção de conexão ao banco de dados;\n",
    "- `DRIVER` é o driver de conexão que o `PySpark` vai usar para conectar com o banco de dados da Oracle;\n",
    "- Vamos usar uma string de conexão `URL`, que usa informações contidas na `TNS_STR`, além do usuário `USR` e senha ` PWD` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WALLET_PLACE = input(\"Insira o caminho completo para a pasta da sua wallet: \")\n",
    "JARS_PLACE = input(\"Insira o caminho completo para a pasta que contém os arqivos .jar necessários: \")\n",
    "\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.jars\", path.join(JARS_PLACE, \"*\"))\n",
    "drivername = \"oracle.jdbc.OracleDriver\"\n",
    "URL = f\"jdbc:oracle:thin:@demodb_high?TNS_ADMIN={WALLET_PLACE}\"\n",
    "USR = \"ADMIN\"\n",
    "PWD = getpass(\"Insira a senha de administrador do banco de dados: \")\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "\t.config(conf=conf)\\\n",
    "\t.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9036b021-6882-4df3-b6b3-1fc206914202",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Agora que temos o `schema` e as **instruções de conexão** prontos, podemos usá-los para ler os dados que obtemos interpretanto os tipos corretamente. Abaixo podemos ver uma amostra das tabelas consolidadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "288019f9-0a64-4410-978a-ab8cb428cf32",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cons_df = s.read.format(\"csv\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"delimiter\", \";\")\\\n",
    "    .option(\"schema\", VAR_TYPES)\\\n",
    "    .csv(\"dbfs:/ans/hosp/2019/CONS/\")\n",
    "\n",
    "cons_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80fee650-ae4c-40c8-ba08-c7a8f32ed24f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "ler depois\n",
    "- https://docs.oracle.com/cloud/help/pt_BR/analytics-cloud/ACSDS/GUID-FB2AEC3B-2178-48DF-8B9F-76ED2D6B5194.htm#ACSDS-GUID-FB2AEC3B-2178-48DF-8B9F-76ED2D6B5194\n",
    "- https://docs.oracle.com/en-us/iaas/autonomous-database-serverless/doc/connect-jdbc-thin-wallet.html#GUID-BE543CFD-6FB4-4C5B-A2EA-9638EC30900D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d95ecb50-4f55-43af-83ba-cc8ab5fa1a53",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tablename = \"hosp_cons_2019\"\n",
    "\n",
    "cons_df.write.format(\"jdbc\")\\\n",
    "\t.option(\"driver\", drivername)\\\n",
    "\t.option(\"url\", URL)\\\n",
    "\t.option(\"dbtable\", tablename )\\\n",
    "\t.option(\"user\", USR)\\\n",
    "\t.option(\"password\", PWD)\\\n",
    "\t.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "549e6e0e-906b-44bc-bb3a-f294cf01491a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "det_df = s.read.format(\"csv\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"delimiter\", \";\")\\\n",
    "    .option(\"schema\", VAR_TYPES)\\\n",
    "    .csv(\"dbfs:/ans/hosp/2019/DET/\")\n",
    "\n",
    "det_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10d1853d-91a2-4f9a-bb4e-ba180b5d3a75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivername = \"oracle.jdbc.OracleDriver\"\n",
    "tablename = \"hosp_det_2019\"\n",
    "\n",
    "det_df.write.format(\"jdbc\")\\\n",
    "\t.option(\"driver\", drivername)\\\n",
    "\t.option(\"url\", URL)\\\n",
    "\t.option(\"dbtable\", tablename )\\\n",
    "\t.option(\"user\", USR)\\\n",
    "\t.option(\"password\", PWD)\\\n",
    "\t.save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4429000962408355,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "collect_routine_ans",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
