{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5ea841a-a163-4369-b46a-7748e0aba90a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Coletando dados de saúde suplementar da ANS\n",
    "Esta rotina de coleta foi feita para ser compatível com ambientes de computação distribuída, para chegar neste objetivo, será usado:\n",
    "\n",
    "- `PySpark` para rotina de coleta e armazenamento de dados\n",
    "- Oracle Cloud Infrastructure (OCI), modalidade gratuita para Data Warehousing\n",
    "\n",
    "Inicialmente, este projeto foi pensado para executar nas instâncias de computação distribuída gratuitas do **Databicks**, mas isto acabou sendo descartado por causa do método de autenticação escolhido para acessar o banco de dados. Para manter o maior nível de segurança, esta rotina será executada localmente.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9649c1e-2a1b-4e7e-8ed1-6e2c0a40a0e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Dependências\n",
    "\n",
    "Vamos instalar um [pacote desenvolvido por mim](https://pypi.org/project/ftp-download/) para superar o desafio de realizar múltiplos downloads de arquivos em servidores usando um código simples, e na sequência vamos instalar as demais dependências, incluindo o PySpark, que será usado aqui para transportar os dados para o banco de dados remoto.\n",
    "\n",
    "Para mais informações sobre como o ambiente de desenvolvimento foi preparado para executar o código abaixo, visite [este artigo](https://medium.com/data-hackers/prepare-seu-ambiente-de-estudos-de-big-data-com-pyspark-e-uma-data-warehouse-na-nuvem-8adacd895ccf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8bd1653-02b8-4e11-bf41-8f4bfc73a265",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %%python3 -m pip install --upgrade -q pip\n",
    "# %%python3 -m pip install -q ftp_download pyspark==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a9effb2-5ca7-4618-a9b0-7af075a70cea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import ftp_download as ftpd\n",
    "from os import path, listdir, makedirs\n",
    "import logging\n",
    "from ftplib import FTP\n",
    "from getpass import getpass\n",
    "from shutil import unpack_archive\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, StructType, StructField\n",
    "\n",
    "spark = SparkSession.builder.appName(\"dbInteract\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebeec307-88d0-4d1f-8a58-adc6cc9855d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Preparando a rotina de coleta\n",
    "\n",
    "Nosso objetivo é conectar à um servidor FTP e baixar uma quantidade considerável de arquivos, o design da rotina abaixo leva em consideração **nossas limitações, que são**:\n",
    "\n",
    "1. Os arquivos são `.csv` compactados dentro de arquivos `.zip`;\n",
    "2. Existem dois tipos de tabelas: \n",
    "  a. *Detalhada*, e\n",
    "  b. *Consolidada*, estas devem compor tabelas diferentes do nosso banco de dados;\n",
    "3. Todos estes arquivos estão separados em pastas, uma para cada estado\n",
    "\n",
    "A **estratégia adotada** consiste em:\n",
    "\n",
    "1. Fazer múltiplos downloads concorrentes usando computação assíncrona;\n",
    "2. Descompactar e organizar os arquivos localmente;\n",
    "3. Usar o pyspark para ler e guardar os dados no banco de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac09effa-27d0-46fe-80bb-bdf769d6915b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/FTP/PDA/TISS/HOSPITALAR/2019/AC', '/FTP/PDA/TISS/HOSPITALAR/2019/AL', '/FTP/PDA/TISS/HOSPITALAR/2019/AM']\n"
     ]
    }
   ],
   "source": [
    "FTP_SERVER = \"ftp.dadosabertos.ans.gov.br\"\n",
    "ROOT_FOLDER_SRC = \"/FTP/PDA/TISS/HOSPITALAR/2019/\"\n",
    "\n",
    "ftp = FTP(FTP_SERVER)\n",
    "ftp.login()\n",
    "remote_paths = ftp.nlst(ROOT_FOLDER_SRC)\n",
    "\n",
    "print(remote_paths[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b782fc3b-b332-410d-adc0-c8b8d508ea0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Já sabemos que os arquivos no servidor possuem nomes regulares, eles possuem sufixo \"DET\" nos arquivos com a tabela detalhada, e \"CONS\" nas tabelas consolidadas. Sabemos também que todos os arquivos estão compactados com extensão `.zip`.\n",
    "\n",
    "Para superar estes dois desafios, vamos criar uma função que identifica o tipo de tabela pelo nome do arquivo e extrai os conteúdos em um diretório em comum, de modo que todas as tabelas consolidadas estejam armazenadas em uma pasta, e todas as tabelas detalhadas estejam armazenadas em uma outra pasta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebaa1e88-e81f-480b-af92-46a98acab855",
     "showTitle": false,
     "title": ""
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_and_organize(search_dir: str, find_patterns=[\"CONS\", \"DET\"]):\n",
    "    \"\"\"\n",
    "    Extrai arquivos `.zip` em pastas, procurando por sequências de strings em\n",
    "    seu nome para definir onde o conteúdo de cada arquivo será extraído. Se um\n",
    "    arquivo contém mais de uma das sequências, ele será extraído em mais de uma\n",
    "    pasta, se um arquivo não contém nenhuma das sequências, ele não será extraído.\n",
    "\n",
    "    ### Parâmetros:\n",
    "\n",
    "    - search_dir (`str`):\n",
    "        Caminho para a pasta com os arquivos.\n",
    "\n",
    "    - find_patterns (`Iterable[str]`):\n",
    "        Uma lista com as sequências que serão procuradas, não é _case sensitive_.\n",
    "    \"\"\"\n",
    "    def filter_by_pattern(pattern, elements):\n",
    "        \"\"\"\n",
    "        Retorna uma lista que contém os nomes dos arquivos que se ajustam à uma\n",
    "        determinada sequência.\n",
    "\n",
    "        ### Parâmetros:\n",
    "\n",
    "        - pattern (`str`):\n",
    "            Sequência que será procurada nos itens de `elements`.\n",
    "        \n",
    "        - elements (`Iterable[str]`):\n",
    "            Lista com itens que serão filtrados sequência `pattern`.\n",
    "\n",
    "        ### Retorna:\n",
    "\n",
    "        (`Iterable[str]`) com os itens que contém a sequência.\n",
    "        \"\"\"\n",
    "        matches = re.compile(pattern, re.IGNORECASE)\n",
    "        return list(filter(matches.search, elements))\n",
    "\n",
    "    filepaths = [\n",
    "        f for f in listdir(search_dir) \n",
    "        if path.isfile(path.join(search_dir, f))\n",
    "    ]\n",
    "    listings = {i:filter_by_pattern(i, filepaths) for i in find_patterns}\n",
    "\n",
    "    for pattern in find_patterns:\n",
    "        destination = path.join(search_dir, pattern)\n",
    "\n",
    "        if not path.exists(destination):\n",
    "            makedirs(destination)\n",
    "\n",
    "        for f in listings[pattern]:\n",
    "            origin = path.join(search_dir, f)\n",
    "            unpack_archive(origin, destination, \"zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da72d120-8cf7-452e-bc92-be96905b07b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A próxima etapa vai envolver o uso de um pacote de minha autoria `ftp_download`, para saber mais sobre o projeto, visite [https://pypi.org/project/ftp-download/](https://pypi.org/project/ftp-download/). A rotina na célula abaixo segue três etapas:\n",
    "\n",
    "1. **Download** dos arquivos para o ambiente local com `ftp_download`\n",
    "2. **Descompactação e organização** dos arquivos baixados em pastas usando a rotina desenvolvida na célula anterior\n",
    "3. **Upload dos arquivos** da unidade local para a base de dados, que será acessada pelo PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bbd8612-52ec-44ec-87b6-9c8f95689a00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Concluído! ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "# exibir log apenas com avisos e erros\n",
    "# se todos os downloads forem um sucesso, veremos apenas\n",
    "# \"=== Concluído! ===\" no stdout\n",
    "ftpd.Conf.verbose = False\n",
    "ftpd.timings.log.handler.setLevel(logging.WARNING)\n",
    "ftpd.timings.log.logger.setLevel(logging.WARNING)\n",
    "\n",
    "for i, rp in enumerate(remote_paths):\n",
    "    print(f\"Progresso: {i/len(remote_paths)*100:.2f}%\", end=\"\\r\")\n",
    "    ftpd.from_folder(ftp, remote_path=rp)\n",
    "print(\"=== Concluído! ===\")\n",
    "\n",
    "with open(ftpd.prefs.LOG_FILE) as logfile:\n",
    "    logfile_contents = logfile.read()\n",
    "    print(logfile_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fedb8e1-0999-4241-a24b-3a73bfbcc2ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2\n",
    "download_place = ftpd.Conf.download_folder\n",
    "extract_and_organize(search_dir=download_place)\n",
    "\n",
    "# 3..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c4b1f66-0414-4479-9890-a252ada0436e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Preparando a rotina de armazenamento\n",
    "\n",
    "Agora que já temos os dados prontos para manipulação, podemos usar o `pyspark` para inserir os dados em nosso banco de dados relacional da Oracle. Para isto, vamos usar usar o [driver JDBC da Oracle](https://www.oracle.com/br/database/technologies/appdev/jdbc-downloads.html). *Esta etapa vai falhar se o driver __não__ estiver instalado no cluster atual*, para efetuar a instalação, seguimos os passos indicados [neste vídeo](https://youtu.be/3tAVXfIBqA8?si=jNeO459775ag9x44&t=261).\n",
    "\n",
    "Vamos começar definindo um `schema` para todos os nomes de colunas possíveis. Para decidir qual o *data type* ideal para cada coluna, usamos o [dicionário de dados](https://dadosabertos.ans.gov.br/FTP/PDA/TISS/DICIONARIO/Dicionario_de_variaveis.ods) fornecido pela ANS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c125cb16-3aaf-459e-b314-09ff7add7cb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CONS_TYPES = StructType([\n",
    "    StructField(\"ID_EVENTO_ATENCAO_SAUDE\", IntegerType(), False),\n",
    "    StructField(\"ID_PLANO\", IntegerType(), True),\n",
    "    StructField(\"FAIXA_ETARIA\", StringType(), True),\n",
    "    StructField(\"SEXO\", StringType(), True),\n",
    "    StructField(\"CD_MUNICIPIO_BENEFICIARIO\", StringType(), True),\n",
    "    StructField(\"PORTE\", StringType(), True),\n",
    "    StructField(\"CD_MODALIDADE\", IntegerType(), True),\n",
    "    StructField(\"NM_MODALIDADE\", StringType(), True),\n",
    "    StructField(\"CD_MUNICIPIO_PRESTADOR\", StringType(), True),\n",
    "    StructField(\"UF_PRESTADOR\", StringType(), True),\n",
    "    StructField(\"TEMPO_DE_PERMANENCIA\", IntegerType(), True),\n",
    "    StructField(\"ANO_MES_EVENTO\", StringType(), True),\n",
    "    StructField(\"CD_CARATER_ATENDIMENTO\", StringType(), True),\n",
    "    StructField(\"CD_TIPO_INTERNACAO\", StringType(), True),\n",
    "    StructField(\"CD_REGIME_INTERNACAO\", StringType(), True),\n",
    "    StructField(\"CD_MOTIVO_SAIDA\", StringType(), True),\n",
    "    StructField(\"CID_1\", StringType(), True),\n",
    "    StructField(\"CID_2\", StringType(), True),\n",
    "    StructField(\"CID_3\", StringType(), True),\n",
    "    StructField(\"CID_4\", StringType(), True),\n",
    "    StructField(\"QT_DIARIA_ACOMPANHANTE\", IntegerType(), True),\n",
    "    StructField(\"QT_DIARIA_UTI\", IntegerType(), True),\n",
    "    StructField(\"IND_ACIDENTE_DOENCA\", StringType(), True),\n",
    "    StructField(\"LG_VALOR_PREESTABELECIDO\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "DET_TYPES = StructType([\n",
    "    StructField(\"ID_EVENTO_ATENCAO_SAUDE\", IntegerType(), False),\n",
    "    StructField(\"UF_PRESTADOR\", StringType(), True),\n",
    "    StructField(\"TEMPO_DE_PERMANENCIA\", IntegerType(), True),\n",
    "    StructField(\"ANO_MES_EVENTO\", StringType(), True),\n",
    "    StructField(\"CD_PROCEDIMENTO\", StringType(), True),\n",
    "    StructField(\"CD_TABELA_REFERENCIA\", StringType(), True),\n",
    "    StructField(\"QT_ITEM_EVENTO_INFORMADO\", IntegerType(), True),\n",
    "    StructField(\"VL_ITEM_EVENTO_INFORMADO\", FloatType(), True),\n",
    "    StructField(\"VL_ITEM_PAGO_FORNECEDOR\", FloatType(), True),\n",
    "    StructField(\"IND_PACOTE\", IntegerType(), True),\n",
    "    StructField(\"IND_TABELA_PROPRIA\", IntegerType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee1754d-e06b-43b7-832f-4dea00a24f15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Variáveis de conexão com o banco de dados\n",
    "Agora vamos obter as variáveis que vão nos ajudar a conectar à *\"autonomous database\"* que já temos alocada na Oracle.\n",
    "\n",
    "- A `TNS_STR` é providenciada pela OCI como uma opção de conexão ao banco de dados;\n",
    "- `DRIVER` é o driver de conexão que o `PySpark` vai usar para conectar com o banco de dados da Oracle;\n",
    "- Vamos usar uma string de conexão `URL`, que usa informações contidas na `TNS_STR`, além do usuário `USR` e senha ` PWD` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Insira a senha de administrador do banco de dados:  ········\n"
     ]
    }
   ],
   "source": [
    "drivername = \"oracle.jdbc.OracleDriver\"\n",
    "WALLET_PLACE = \"/home/user/Downloads/wallet_demodb\"\n",
    "URL = f\"jdbc:oracle:thin:@demodb_medium?TNS_ADMIN={WALLET_PLACE}\"\n",
    "USR = \"ADMIN\"\n",
    "PWD = getpass(\"Insira a senha de administrador do banco de dados: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9036b021-6882-4df3-b6b3-1fc206914202",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Agora que temos o `schema` prontos, podemos usá-los para ler os dados que obtemos interpretanto os tipos corretamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "288019f9-0a64-4410-978a-ab8cb428cf32",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cons_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"delimiter\", \";\")\\\n",
    "    .schema(CONS_TYPES)\\\n",
    "    .csv(f\"{download_place}/CONS/\")\n",
    "\n",
    "det_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"delimiter\", \";\")\\\n",
    "    .schema(DET_TYPES)\\\n",
    "    .csv(f\"{download_place}/DET/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80fee650-ae4c-40c8-ba08-c7a8f32ed24f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A última parte desta etapa é usar as **variáveis de conexão** para enviar todos os dados para a nossa Data Warehouse: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d95ecb50-4f55-43af-83ba-cc8ab5fa1a53",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tablename = \"HOSP_CONS\"\n",
    "\n",
    "cons_df.write.format(\"jdbc\")\\\n",
    "    .option(\"driver\", drivername)\\\n",
    "    .option(\"dbtable\", tablename )\\\n",
    "    .option(\"url\", URL)\\\n",
    "    .option(\"user\", USR)\\\n",
    "    .option(\"password\", PWD)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10d1853d-91a2-4f9a-bb4e-ba180b5d3a75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tablename = \"HOSP_DET\"\n",
    "\n",
    "det_df.write.format(\"jdbc\")\\\n",
    "    .option(\"driver\", drivername)\\\n",
    "    .option(\"dbtable\", tablename )\\\n",
    "    .option(\"url\", URL)\\\n",
    "    .option(\"user\", USR)\\\n",
    "    .option(\"password\", PWD)\\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4429000962408355,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "collect_routine_ans",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
