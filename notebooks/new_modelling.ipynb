{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fbd8ca1-60ad-4252-a965-0688e321d162",
   "metadata": {},
   "source": [
    "# Modelagem de classificação de clientes com dados da saúde suplementar\n",
    "\n",
    "Vamos usar os [dados obtidos anteriormente](https://github.com/VFLins/hospitalar_data_ans/blob/main/notebooks/collect_routine_ans.ipynb) e o conhecimento que obtivemos na sua [análise exploratória](https://vflins.github.io/hospitalar_data_ans/pages/eda.html) para produzir uma solução de dados com valor de negócio para empresas do **setor de operação de saúde.**\n",
    "\n",
    "A proposta deste modelo é produzir um indicador do *\"custo potencial de aquisição\"* de um cliente. Esta solução pode facilitar grandemente a precificação dos produtos de saúde, além disso, neste processo podemos acabar aprendendo mais sobre os diferentes tipos de perfis de clientes e suas necessidades!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b674eb91-bfb8-4fd4-b431-6626aa184142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import logging\n",
    "loghandler = logging.FileHandler(\".log\")\n",
    "logging.getLogger().addHandler(loghandler)\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from getpass import getpass\n",
    "from pyspark.sql.types import StructType, StructField, TimestampNTZType, DateType, FloatType, StringType\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f113e2f3-e354-4d26-9f89-f8c3252ac21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark session\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced139e5-a16a-45ba-aed4-79bd7b12647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Insira a senha de administrador do banco de dados:  ········\n"
     ]
    }
   ],
   "source": [
    "# variaveis de conexao\n",
    "\n",
    "drivername = \"oracle.jdbc.OracleDriver\"\n",
    "WALLET_PLACE = \"/home/user/Downloads/wallet_demodb\"\n",
    "URL = f\"jdbc:oracle:thin:@demodb_medium?TNS_ADMIN={WALLET_PLACE}\"\n",
    "USR = \"ADMIN\"\n",
    "PWD = getpass(\"Insira a senha de administrador do banco de dados: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8134f53-61d6-4970-b514-6a09f98a116e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/21 11:24:45 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)/ 1]\n",
      "java.sql.SQLException: ORA-01652: unable to extend temp segment by 128 in tablespace TEMP\n",
      "\n",
      "https://docs.oracle.com/error-help/db/ora-01652/\n",
      "\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:702)\n",
      "\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:608)\n",
      "\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1330)\n",
      "\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:1102)\n",
      "\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:456)\n",
      "\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:498)\n",
      "\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:272)\n",
      "\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:1198)\n",
      "\tat oracle.jdbc.driver.OracleStatement.prepareDefineBufferAndExecute(OracleStatement.java:1390)\n",
      "\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:1269)\n",
      "\tat oracle.jdbc.driver.OracleStatement.executeSQLSelect(OracleStatement.java:1794)\n",
      "\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1592)\n",
      "\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3754)\n",
      "\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:3940)\n",
      "\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1103)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:275)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: Error : 1652, Position : 256, SQL = SELECT \"ID_EVENTO_ATENCAO_SAUDE\",\"UF_PRESTADOR\",\"TEMPO_DE_PERMANENCIA\",\"ANO_MES_EVENTO\",\"CD_PROCEDIMENTO\",\"CD_TABELA_REFERENCIA\",\"QT_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_PAGO_FORNECEDOR\",\"IND_PACOTE\",\"IND_TABELA_PROPRIA\" FROM (SELECT * FROM HOSP_DET ORDER BY dbms_random.value FETCH FIRST 4016936 ROWS ONLY) SPARK_GEN_SUBQ_15    , Original SQL = SELECT \"ID_EVENTO_ATENCAO_SAUDE\",\"UF_PRESTADOR\",\"TEMPO_DE_PERMANENCIA\",\"ANO_MES_EVENTO\",\"CD_PROCEDIMENTO\",\"CD_TABELA_REFERENCIA\",\"QT_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_PAGO_FORNECEDOR\",\"IND_PACOTE\",\"IND_TABELA_PROPRIA\" FROM (SELECT * FROM HOSP_DET ORDER BY dbms_random.value FETCH FIRST 4016936 ROWS ONLY) SPARK_GEN_SUBQ_15    , Error Message = ORA-01652: unable to extend temp segment by 128 in tablespace TEMP\n",
      "\n",
      "\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:710)\n",
      "\t... 34 more\n",
      "24/04/21 11:24:45 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o45.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (172.18.59.98 executor driver): java.sql.SQLException: ORA-01652: unable to extend temp segment by 128 in tablespace TEMP\n\nhttps://docs.oracle.com/error-help/db/ora-01652/\n\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:702)\n\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:608)\n\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1330)\n\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:1102)\n\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:456)\n\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:498)\n\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:272)\n\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:1198)\n\tat oracle.jdbc.driver.OracleStatement.prepareDefineBufferAndExecute(OracleStatement.java:1390)\n\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:1269)\n\tat oracle.jdbc.driver.OracleStatement.executeSQLSelect(OracleStatement.java:1794)\n\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1592)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3754)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:3940)\n\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:275)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: Error : 1652, Position : 256, SQL = SELECT \"ID_EVENTO_ATENCAO_SAUDE\",\"UF_PRESTADOR\",\"TEMPO_DE_PERMANENCIA\",\"ANO_MES_EVENTO\",\"CD_PROCEDIMENTO\",\"CD_TABELA_REFERENCIA\",\"QT_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_PAGO_FORNECEDOR\",\"IND_PACOTE\",\"IND_TABELA_PROPRIA\" FROM (SELECT * FROM HOSP_DET ORDER BY dbms_random.value FETCH FIRST 4016936 ROWS ONLY) SPARK_GEN_SUBQ_15    , Original SQL = SELECT \"ID_EVENTO_ATENCAO_SAUDE\",\"UF_PRESTADOR\",\"TEMPO_DE_PERMANENCIA\",\"ANO_MES_EVENTO\",\"CD_PROCEDIMENTO\",\"CD_TABELA_REFERENCIA\",\"QT_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_PAGO_FORNECEDOR\",\"IND_PACOTE\",\"IND_TABELA_PROPRIA\" FROM (SELECT * FROM HOSP_DET ORDER BY dbms_random.value FETCH FIRST 4016936 ROWS ONLY) SPARK_GEN_SUBQ_15    , Error Message = ORA-01652: unable to extend temp segment by 128 in tablespace TEMP\n\n\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:710)\n\t... 34 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4148)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4145)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.sql.SQLException: ORA-01652: unable to extend temp segment by 128 in tablespace TEMP\n\nhttps://docs.oracle.com/error-help/db/ora-01652/\n\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:702)\n\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:608)\n\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1330)\n\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:1102)\n\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:456)\n\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:498)\n\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:272)\n\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:1198)\n\tat oracle.jdbc.driver.OracleStatement.prepareDefineBufferAndExecute(OracleStatement.java:1390)\n\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:1269)\n\tat oracle.jdbc.driver.OracleStatement.executeSQLSelect(OracleStatement.java:1794)\n\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1592)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3754)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:3940)\n\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:275)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: Error : 1652, Position : 256, SQL = SELECT \"ID_EVENTO_ATENCAO_SAUDE\",\"UF_PRESTADOR\",\"TEMPO_DE_PERMANENCIA\",\"ANO_MES_EVENTO\",\"CD_PROCEDIMENTO\",\"CD_TABELA_REFERENCIA\",\"QT_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_PAGO_FORNECEDOR\",\"IND_PACOTE\",\"IND_TABELA_PROPRIA\" FROM (SELECT * FROM HOSP_DET ORDER BY dbms_random.value FETCH FIRST 4016936 ROWS ONLY) SPARK_GEN_SUBQ_15    , Original SQL = SELECT \"ID_EVENTO_ATENCAO_SAUDE\",\"UF_PRESTADOR\",\"TEMPO_DE_PERMANENCIA\",\"ANO_MES_EVENTO\",\"CD_PROCEDIMENTO\",\"CD_TABELA_REFERENCIA\",\"QT_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_PAGO_FORNECEDOR\",\"IND_PACOTE\",\"IND_TABELA_PROPRIA\" FROM (SELECT * FROM HOSP_DET ORDER BY dbms_random.value FETCH FIRST 4016936 ROWS ONLY) SPARK_GEN_SUBQ_15    , Error Message = ORA-01652: unable to extend temp segment by 128 in tablespace TEMP\n\n\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:710)\n\t... 34 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     17\u001b[0m tablename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHOSP_DET\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtablename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ORDER BY dbms_random.value FETCH FIRST 4016936 ROWS ONLY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m det_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrivername\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mURL\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUSR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPWD\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/py/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[1;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/envs/py/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1261\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m \n\u001b[1;32m   1243\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1261\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/envs/py/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/envs/py/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/envs/py/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o45.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (172.18.59.98 executor driver): java.sql.SQLException: ORA-01652: unable to extend temp segment by 128 in tablespace TEMP\n\nhttps://docs.oracle.com/error-help/db/ora-01652/\n\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:702)\n\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:608)\n\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1330)\n\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:1102)\n\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:456)\n\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:498)\n\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:272)\n\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:1198)\n\tat oracle.jdbc.driver.OracleStatement.prepareDefineBufferAndExecute(OracleStatement.java:1390)\n\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:1269)\n\tat oracle.jdbc.driver.OracleStatement.executeSQLSelect(OracleStatement.java:1794)\n\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1592)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3754)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:3940)\n\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:275)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: Error : 1652, Position : 256, SQL = SELECT \"ID_EVENTO_ATENCAO_SAUDE\",\"UF_PRESTADOR\",\"TEMPO_DE_PERMANENCIA\",\"ANO_MES_EVENTO\",\"CD_PROCEDIMENTO\",\"CD_TABELA_REFERENCIA\",\"QT_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_PAGO_FORNECEDOR\",\"IND_PACOTE\",\"IND_TABELA_PROPRIA\" FROM (SELECT * FROM HOSP_DET ORDER BY dbms_random.value FETCH FIRST 4016936 ROWS ONLY) SPARK_GEN_SUBQ_15    , Original SQL = SELECT \"ID_EVENTO_ATENCAO_SAUDE\",\"UF_PRESTADOR\",\"TEMPO_DE_PERMANENCIA\",\"ANO_MES_EVENTO\",\"CD_PROCEDIMENTO\",\"CD_TABELA_REFERENCIA\",\"QT_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_PAGO_FORNECEDOR\",\"IND_PACOTE\",\"IND_TABELA_PROPRIA\" FROM (SELECT * FROM HOSP_DET ORDER BY dbms_random.value FETCH FIRST 4016936 ROWS ONLY) SPARK_GEN_SUBQ_15    , Error Message = ORA-01652: unable to extend temp segment by 128 in tablespace TEMP\n\n\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:710)\n\t... 34 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4148)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4145)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.sql.SQLException: ORA-01652: unable to extend temp segment by 128 in tablespace TEMP\n\nhttps://docs.oracle.com/error-help/db/ora-01652/\n\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:702)\n\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:608)\n\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1330)\n\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:1102)\n\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:456)\n\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:498)\n\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:272)\n\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:1198)\n\tat oracle.jdbc.driver.OracleStatement.prepareDefineBufferAndExecute(OracleStatement.java:1390)\n\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:1269)\n\tat oracle.jdbc.driver.OracleStatement.executeSQLSelect(OracleStatement.java:1794)\n\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1592)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3754)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:3940)\n\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:275)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: Error : 1652, Position : 256, SQL = SELECT \"ID_EVENTO_ATENCAO_SAUDE\",\"UF_PRESTADOR\",\"TEMPO_DE_PERMANENCIA\",\"ANO_MES_EVENTO\",\"CD_PROCEDIMENTO\",\"CD_TABELA_REFERENCIA\",\"QT_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_PAGO_FORNECEDOR\",\"IND_PACOTE\",\"IND_TABELA_PROPRIA\" FROM (SELECT * FROM HOSP_DET ORDER BY dbms_random.value FETCH FIRST 4016936 ROWS ONLY) SPARK_GEN_SUBQ_15    , Original SQL = SELECT \"ID_EVENTO_ATENCAO_SAUDE\",\"UF_PRESTADOR\",\"TEMPO_DE_PERMANENCIA\",\"ANO_MES_EVENTO\",\"CD_PROCEDIMENTO\",\"CD_TABELA_REFERENCIA\",\"QT_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_EVENTO_INFORMADO\",\"VL_ITEM_PAGO_FORNECEDOR\",\"IND_PACOTE\",\"IND_TABELA_PROPRIA\" FROM (SELECT * FROM HOSP_DET ORDER BY dbms_random.value FETCH FIRST 4016936 ROWS ONLY) SPARK_GEN_SUBQ_15    , Error Message = ORA-01652: unable to extend temp segment by 128 in tablespace TEMP\n\n\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:710)\n\t... 34 more\n"
     ]
    }
   ],
   "source": [
    "# conjuntos de dados\n",
    "\n",
    "tablename = \"HOSP_CONS\"\n",
    "query = f\"SELECT * FROM {tablename} ORDER BY dbms_random.value FETCH FIRST 624118 ROWS ONLY\"\n",
    "\n",
    "cons_df = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"driver\", drivername)\\\n",
    "    .option(\"query\", query)\\\n",
    "    .option(\"url\", URL)\\\n",
    "    .option(\"user\", USR)\\\n",
    "    .option(\"password\", PWD)\\\n",
    "    .load()\\\n",
    "    .toPandas()\n",
    "\n",
    "\n",
    "tablename = \"HOSP_DET\"\n",
    "query = f\"SELECT * FROM {tablename} ORDER BY dbms_random.value FETCH FIRST 4016936 ROWS ONLY\"\n",
    "\n",
    "det_df = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"driver\", drivername)\\\n",
    "    .option(\"query\", query)\\\n",
    "    .option(\"url\", URL)\\\n",
    "    .option(\"user\", USR)\\\n",
    "    .option(\"password\", PWD)\\\n",
    "    .load()\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00745c1b-be7b-4800-8d3c-f644b14324a1",
   "metadata": {},
   "source": [
    "# 1 Criando um *target*\n",
    "\n",
    "Vamos abordar este problema usando um modelo de classificação. Para isto vamos precisar de um variável target binária que separe os clientes entre \"alto custo\" e \"baixo custo\". Usaremos a probabilidade estimada de um cliente pertencer a um grupo ou à outro como o *score* que indica seu custo potencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ac6e82-8486-4219-b6fc-399d1d06aae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidada:\n",
      "['ID_EVENTO_ATENCAO_SAUDE', 'ID_PLANO', 'FAIXA_ETARIA', 'SEXO', 'CD_MUNICIPIO_BENEFICIARIO', 'PORTE', 'CD_MODALIDADE', 'NM_MODALIDADE', 'CD_MUNICIPIO_PRESTADOR', 'UF_PRESTADOR', 'TEMPO_DE_PERMANENCIA', 'ANO_MES_EVENTO', 'CD_CARATER_ATENDIMENTO', 'CD_TIPO_INTERNACAO', 'CD_REGIME_INTERNACAO', 'CD_MOTIVO_SAIDA', 'CID_1', 'CID_2', 'CID_3', 'CID_4', 'QT_DIARIA_ACOMPANHANTE', 'QT_DIARIA_UTI', 'IND_ACIDENTE_DOENCA', 'LG_VALOR_PREESTABELECIDO']\n",
      "\n",
      "Detalhada:\n",
      "['ID_EVENTO_ATENCAO_SAUDE', 'UF_PRESTADOR', 'TEMPO_DE_PERMANENCIA', 'ANO_MES_EVENTO', 'CD_PROCEDIMENTO', 'CD_TABELA_REFERENCIA', 'QT_ITEM_EVENTO_INFORMADO', 'VL_ITEM_EVENTO_INFORMADO', 'VL_ITEM_PAGO_FORNECEDOR', 'IND_PACOTE', 'IND_TABELA_PROPRIA']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Consolidada:\\n{cons_df.schema.names}\", f\"Detalhada:\\n{det_df.schema.names}\", sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672727d-f735-4720-a467-5f5e839f2ef8",
   "metadata": {},
   "source": [
    "## 1.1 A especificidade de cada conjunto de dados\n",
    "\n",
    "Para obter o custo de um cliente, seria ideal ter um conjunto de dados que representa um cliente único em cada linha, em vez disto, temos:\n",
    "\n",
    "- A nossa base de dados **consolidada** apresentando um atendimento médico para cada linha\n",
    "- A base de dados **detalhada** apresentando um procedimento do atendimento médico para cada linha\n",
    "\n",
    "Podemos perceber que as variáveis que descrevem o cliente (exemplo: `\"FAIXA_ETARIA\"`, `\"SEXO\"`, etc.) estão presentes na tabela *consolidada*. Para obter um conjunto no formato que precisamos para modelagem, será necessário agregar algumas características dos consumidores presentes na tabela consolidada.\n",
    "\n",
    "Depois que tivermos como \"sinalizar\" quais são os clientes únicos presentes na tabela **consolidada**, podemos extrair mais informações sobre seus respctivos eventos hospitalares na tabela **detalhada**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2d3f21-60f5-449f-ab92-c0040cabc987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "from os import makedirs\n",
    "import requests\n",
    "\n",
    "\n",
    "response = requests.get(\"https://archive.ics.uci.edu/static/public/34/diabetes.zip\")\n",
    "datafile = \"diabetes-data.tar.Z\"\n",
    "target_path = f\"/home/user/code/hospitalar_data_ans/data/\"\n",
    "makedirs(target_path, exist_ok=True)\n",
    "\n",
    "with ZipFile(BytesIO(response.content)) as zipf:\n",
    "    if datafile in zipf.namelist():\n",
    "        with zipf.open(datafile) as origin:\n",
    "            with open(f\"{target_path}/data.txt\", \"wb\") as target:\n",
    "                target.write(origin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4c6900b-2154-41f2-9ddb-f1f4781070a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"id\", monotonically_increasing_id() + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
